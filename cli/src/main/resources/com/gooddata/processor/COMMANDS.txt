Usage: gdi.sh -u username -p password [ -h hostname ] [ -i project_id ] [-e commands] [<file1>, ...]
 -h,--host <arg>       GoodData host (secure.gooddata.com by default)
 -f, --ftphost <arg>   GoodData FTP host (secure-di.gooddata.com by default)
 -t, --proto <arg>     Protocol to access GoodData (HTTP or HTTPS, HTTPS by default)
 -s, --insecure        Disable encryption for HTTP and FTP (prefer this to -t)
 -p,--password <arg>   GoodData password
 -u,--username <arg>   GoodData username
 -i,--project <arg>    GoodData project identifier (takes the form of an com.gooddata.MD5 hash)
 -e,--execute <arg>    Commands to execute
 -V, --version         Prints out the tool version
 file                  path to script file with commands to execute
 
Either -e or file(s) must be specified.

Project Management Commands:
----------------------------

 CreateProject(name=<project-name>, desc=<description>, templateUri=<templateUri>) - create a new project on the <hostname> server
  project-name - name of the new project
  description  - (optional) project description
  templateUri - (optional) project template to create the project from

 DeleteProject(id=<project-id>) - drop the project on the <hostname> server
  project-id - optional project id, if not specified, the command tries to drop the current project

 OpenProject(id=<identifier>) - open an existing project for data modeling and data upload. Equivalent to providing the project identifier using the "-e" command line option.
  identifier - id of an existing project (takes the form of an com.gooddata.MD5 hash)

 RememberProject(fileName=<file>) - saves the current project identifier into the specified file
  fileName - file to save the project identifier
  
 UseProject(fileName=<file>) - loads the current project identifier from the specified file
  fileName - file to load the project identifier from

 InviteUser(email=<email>, msg=<msg>[, role=<admin|editor|dashboard only>]) - invites a new user to the project, must call CreateProject or OpenProject before
  email - the invited user's e-mail
  msg - optional invitation message
  role - initial user's role

 RetrieveAllObjects(dir=<directory-to-store-the-objects>) - retrieves all metadata objects from the current project and stores it in a directory, must call CreateProject or OpenProject before
  dir - directory where the objects content (JSON) are going to be stored

 RetrieveMetadataObject(id=<object-id>, file=<file-to-store-the-object>) - retrieves a metadata object and stores it in a file, must call CreateProject or OpenProject before
  object-id - valid object id (integer number)
  file - file where the object content (JSON) is going to be stored

 StoreMetadataObject([id=<object-id>,] file=<file-with-the-object-content>) - stores a metadata object with a content (JSON) in file to the metadata server, must call CreateProject or OpenProject before
  object-id - valid object id (integer number), if the id is specified, the object is going to be modified, if not, a new object is created
  file - file where the object content (JSON) is stored

 DropMetadataObject(id=<object-id>) - drops the object with specified id from the project's metadata, must call CreateProject or OpenProject before
  object-id - valid object id (integer number)

 StoreAllObjects(dir=<directory-with-the-objects>, overwrite=<overwrite-flag>) - copies all metadata objects from the directory to the current project, must call CreateProject or OpenProject before
   dir - directory where the copied objects content (JSON) are stored
   overwrite - if true overwrite the existing objects in the project

 Lock(path=<file>) - prevents concurrent run of multiple instances sharing the same lock file. Lock files older than 1 hour are discarded.

 MigrateDatasets(configFiles=<comma-separated-list-of-configs>) - migrates the project's datasets from CL 1.1.x to CL 1.2.x, , must call CreateProject or OpenProject before
  configFiles - the comma separated list of ALL project's dataset's XML configuration files

Logical Model Management Commands:
----------------------------------

 GenerateMaql(maqlFile=<maql>) - generate MAQL DDL script describing data model from the local config file, must call CreateProject or OpenProject and a UseXXX before
  maqlFile - path to MAQL file (will be overwritten)
  
 GenerateUpdateMaql(maqlFile=<maql>) - generate MAQL DDL alter script that creates the columns available in the local configuration but missing in the remote GoodData project, must call CreateProject or OpenProject and UseXXX before
  maqlFile - path to MAQL file (will be overwritten)

 ExecuteMaql(maqlFile=<maql> [, ifExists=<true | false>]) - run MAQL DDL script on server to generate data model, must call CreateProject or OpenProject and UseXXX before
  maqlFile - path to the MAQL file (relative to PWD)
  ifExists - if set to true the command quits silently if the maqlFile does not exist, default is false

Data Transfer Commands:
-----------------------

 TransferAllSnapshots([incremental=<true | false>] [, waitForFinish=<true | false>]) - upload data (all snapshots) to the GoodData server, must call CreateProject or OpenProject and Load<Connector> before. Not allowed for data set defining a connection point unless only one snapshot is present.
  incremental - incremental transfer (true | false), default is false
  waitForFinish - waits for the server-side processing (true | false), default is true

 TransferSnapshots(firstSnapshot=snapshot-id, lastSnapshot=snapshot-id [,incremental=<true | false>] [, waitForFinish=<true | false>]) - uploads all snapshots between the firstSnapshot and the lastSnapshot (inclusive). Only one snapshot is allowed for data set defining a connection point. 
  firstSnapshot - the first transferred snapshot id
  lastSnapshot - the last transferred snapshot id
  incremental - incremental transfer (true | false), default is false
  waitForFinish - waits for the server-side processing (true | false), default is true

 TransferLastSnapshot([incremental=<true | false>] [, waitForFinish=<true | false>]) ) - upload the last snapshot to the GoodData server
  incremental - incremental transfer (true | false), default is false
  waitForFinish - waits for the server-side processing (true | false), default is true

CSV Connector Commands:
-----------------------

 GenerateCsvConfig(csvHeaderFile=<file>, configFile=<config> [, defaultLdmType=<mode>] [, folder=<folder>], [separator = <separator-char>]) - generate a sample XML config file based on the fields from your CSV file. If the config file exists already, only new columns are added. The config file must be edited as the LDM types (attribute | fact | label etc.) are assigned randomly.
  csvHeaderFile - path to CSV file (only the first header row will be used)
  configFile  - path to configuration file (will be overwritten)
  defaultLdmType - LDM mode to be associated with new columns (only ATTRIBUTE mode is supported by the ProcessNewColumns task at this time)
  folder - folder where to place new attributes
  separator - optional field separator, the default is ','  

 UseCsv(csvDataFile=<data>, configFile=<config> [, hasHeader=<true | false>] [, separator = <separator-char>]) - load CSV data file using config file describing the file structure, must call CreateProject or OpenProject before
  csvDataFile    - path to CSV datafile
  configFile  - path to XML configuration file (see the GenerateCsvConfig command that generates the config file template)
  hasHeader - true if the CSV file has a header row (default is true)
  separator - optional field separator, the default is ','. Use '\t' or type the tab char for tabulator.

GoogleAnalytics Connector Commands:
-----------------------------------

 GenerateGoogleAnalyticsConfig(name=<name>, configFile=<config>, dimensions=<pipe-separated-ga-dimensions>, metrics=<pipe-separated-ga-metrics>) - generate an XML config file based on the fields from your GA query.
  name - the new dataset name
  configFile  - path to configuration file (will be overwritten)
  dimensions - pipe (|) separated list of Google Analytics dimensions (see http://code.google.com/apis/analytics/docs/gdata/gdataReferenceDimensionsMetrics.html)
  metrics - pipe (|) separated list of Google Analytics metrics (see http://code.google.com/apis/analytics/docs/gdata/gdataReferenceDimensionsMetrics.html)


 UseGoogleAnalytics(configFile=<config>, username=<ga-username>, password=<ga-password>, profileId=<ga-profile-id>, dimensions=<pipe-separated-ga-dimensions>, metrics=<pipe-separated-ga-metrics>, startDate=<date>, endDate=<date>, filters=<ga-filter-string>)  - load GA data file using config file describing the file structure, must call CreateProject or OpenProject before
  configFile  - path to configuration file (will be overwritten)
  token - Google Analytics AuthSub token (you must specify either the token or username/password)
  username - Google Analytics username (you must specify either the token or username/password)
  password - Google Analytics password (you must specify either the token or username/password)
  profileId - Google Analytics profile ID (this is a value of the id query parameter in the GA url)
  dimensions - pipe (|) separated list of Google Analytics dimensions (see http://code.google.com/apis/analytics/docs/gdata/gdataReferenceDimensionsMetrics.html)
  metrics - pipe (|) separated list of Google Analytics metrics (see http://code.google.com/apis/analytics/docs/gdata/gdataReferenceDimensionsMetrics.html)
  startDate - the GA start date in the yyyy-mm-dd format  
  endDate - the GA end date in the yyyy-mm-dd format  
  filters - the GA filters (see http://code.google.com/apis/analytics/docs/gdata/gdataReferenceDataFeed.html#filters)

JDBC Connector Commands:
------------------------

 GenerateJdbcConfig(name=<name>, configFile=<config>, driver=<jdbc-driver>, url=<jdbc-url>, query=<sql-query> [, username=<jdbc-username>] [, password=<jdbc-password>])  - generate an XML config file based on the fields from your JDBC query.
  name - the new dataset name
  configFile  - path to configuration file (will be overwritten)
  driver - JDBC driver string (e.g. "org.apache.derby.jdbc.EmbeddedDriver"), you'll need to place the JAR with the JDBC driver to the lib subdirectory
  url - JDBC url (e.g. "jdbc:derby:mydb")
  query - SQL query (e.g. "SELECT employee,dept,salary FROM payroll")
  username - JDBC username
  password - JDBC password
  
 UseJdbc(configFile=<config>, driver=<jdbc-driver>, url=<jdbc-url>, query=<sql-query> [, username=<jdbc-username>] [, password=<jdbc-password>])  - load JDBC data file using config file describing the file structure, must call CreateProject or OpenProject before
  configFile  - path to configuration file (will be overwritten)
  driver - JDBC driver string (e.g. "org.apache.derby.jdbc.EmbeddedDriver"), you'll need to place the JAR with the JDBC driver to the lib subdirectory
  url - JDBC url (e.g. "jdbc:derby:mydb")
  query - SQL query (e.g. "SELECT employee,dept,salary FROM payroll")
  username - JDBC username
  password - JDBC password

 ExportJdbcToCsv(dir=<dir>, driver=<jdbc-driver>, url=<jdbc-url> [, username=<jdbc-username>] [, password=<jdbc-password>])  - exports all tables from the database to CSV file
  dir - target directory
  driver - JDBC driver string (e.g. "org.apache.derby.jdbc.EmbeddedDriver"), you'll need to place the JAR with the JDBC driver to the lib subdirectory
  url - JDBC url (e.g. "jdbc:derby:mydb")
  username - JDBC username
  password - JDBC password

SalesForce Connector Commands:
------------------------------

 GenerateSfdcConfig(name=<name>, configFile=<config>, query=<soql-query>, username=<sfdc-username>, password=<sfdc-password>, token=<sfdc-security-token>)  - generate an XML config file based on the fields from your SFDC query.
  name - the new dataset name
  configFile  - path to configuration file
  query - SOQL query (e.g. "SELECT Id, Name FROM Account"), see http://www.salesforce.com/us/developer/docs/api/Content/data_model.htm
  username - SFDC username
  password - SFDC password
  token - SFDC security token (you may append the security token to the password instead using this parameter)
  
 UseSfdc(configFile=<config>, query=<soql-query>, username=<sfdc-username>, password=<sfdc-password>, token=<sfdc-security-token>)  - load SalesForce data file using config file describing the file structure, must call CreateProject or OpenProject before
  configFile  - path to configuration file (will be overwritten)
  query - SOQL query (e.g. "SELECT Id, Name FROM Account"), see http://www.salesforce.com/us/developer/docs/api/Content/data_model.htm
  username - SFDC username
  password - SFDC password
  token - SFDC security token (you may append the security token to the password instead using this parameter)

SalesForce Sales Connector Commands:
------------------------------------

 UseSfdcSales(username=<sfdc-username>, password=<sfdc-password>, token=<sfdc-security-token>, accountQuery=<account-query>, userQuery=<user-query>, opportunityQuery=<opportunity-query>, snapshotQuery=<snapshot-query>, accountConfigFile=<account-config>, userConfigFile=<user-config>, opportunityConfigFile=<opportunity-config>, snapshotConfigFile=<snapshot-config>)  - load SalesForce sales data, must call CreateProject or OpenProject before
  username - SFDC username
  password - SFDC password
  token - SFDC security token (you may append the security token to the password instead using this parameter)
  accountQuery - Account SOQL query (must contain Id, e.g. "SELECT Id, Name FROM Account"), see http://www.salesforce.com/us/developer/docs/api/Content/data_model.htm
  userQuery - Account SOQL query (must contain Id, e.g. "SELECT Id, Name FROM User"), see http://www.salesforce.com/us/developer/docs/api/Content/data_model.htm
  opportunityQuery - Opportunity SOQL query (must contain Id, e.g. "SELECT Id, Name FROM Opportunity"), see http://www.salesforce.com/us/developer/docs/api/Content/data_model.htm
  snapshotQuery - Opportunity snapshot SOQL query (must contain Id, AccountId, and OwnerId, e.g. "SELECT Id, AccountId, OwnerId, Name FROM Opportunity"), see http://www.salesforce.com/us/developer/docs/api/Content/data_model.htm
  accountConfigFile - Account XML schema config file
  userConfigFile - User XML schema config file
  opportunityConfigFile - Opportunity XML schema config file
  snapshotConfigFile - Opportunity snapshot XML schema config file


Pivotal Tracker Connector Commands:
------------------------------

 UsePivotalTracker(username=<pt-username>, password=<pt-password>, pivotalProjectId=<pt-project-id>, storyConfigFile=<story-config>, labelConfigFile=<label-config>, labelToStoryConfigFile=<label-to-story-config>, snapshotConfigFile=<snapshot-config>) - downloads and transforms the PT data.
  username - PT username
  password - PT password
  pivotalProjectId - PT project ID (integer)
  storyConfigFile - PT stories XML schema config
  labelConfigFile - PT labels XML schema config
  labelToStoryConfigFile - PT labels to stories XML schema config
  snapshotConfigFile - PT snapshots XML schema config 

Time Dimension Connector Commands:
----------------------------------

 UseDateDimension(name=<name>, includeTime = <true | false>)  - load new time dimension into the project, must call CreateProject or OpenProject before
  name - the time dimension name differentiates the time dimension form others. This is typically something like "closed", "created" etc.
  includeTime - generate the time dimension  
